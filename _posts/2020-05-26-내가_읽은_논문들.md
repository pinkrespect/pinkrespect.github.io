---
title: 내가 읽은 논문 소개 
published: true
---

## [](#header-1)서론
이력서를 다시 쓰면서, 지금의 개인 컴퓨터나 노트북도 지원 해 주지 않는 회사에서 탈출을 준비 중이다.<br>
그런데 뭔가 부족하다는 생각이 들어서 보니 여태 내가 공부했던 논문에 대한 요약을 소개하는 것이 늦었다는 생각이 들어<br>
급하게 글을 마련했다. 바보 같은 나 ㅇㅅㅇ)..<br>

## [](#header-3)읽은 논문
### [](#header-3-1)요약된 논문
깃헙에 올린 논문 요약본들은 대부분 이미지 링크가 깨져서, 나중에 수정해야겠다.<br>

#### FPGA 관련 논문
1. [AI-CP01. Deep Learning on FPGAs 요약.md](https://github.com/pinkrespect/papers_I_read/blob/master/AI-CP01.%20Deep%20Learning%20on%20FPGAs%20%EC%9A%94%EC%95%BD.md)
  - FPGA 관련 논문이며, Deep Learning을 FPGA(Field Programmable Gate Array)에 어떻게 올릴지에 대한 논문
  - 친 환경적인 딥러닝을 어떻게 하면 좋을까..에 대한 관심이 있었는데 그 답을 FPGA에서 찾을 수 있음

2. [AI-CP09. Application of FPGA to real-time ML.md](https://github.com/pinkrespect/papers_I_read/blob/master/AI-CP09.%20Application%20of%20FPGA%20to%20real-time%20ML.md)
  - 이 논문 정말 어려웠음
  - 축적 컴퓨터에 대한 기본 개념이 있어야함

3. [[DL] A survey of fpga-based neural network inference accelerator.md](https://github.com/pinkrespect/papers_I_read/blob/master/%5BDL%5D%20A%20survey%20of%20fpga-based%20neural%20network%20inference%20accelerator.md)

#### AI Compression 관련 논문
1. [Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman coding](https://github.com/pinkrespect/papers_I_read/blob/master/AI-Comp%2001%20%EC%9A%94%EC%95%BD%20-%2020200103%20%EA%B9%80%EC%96%91%EC%84%A0.md)
  - Compression에 필요한 세가지 주요 아이디어를 소개

2. [EIE: Efficient Inference Engine on Compressed Deep Neural Network](https://github.com/pinkrespect/papers_I_read/blob/master/AI-Comp%2002%20%EC%9A%94%EC%95%BD%20-%2020200103%20%EA%B9%80%EC%96%91%EC%84%A0.md)
  - Compressed Deep Neural Network의 성능에 관련된 논문

#### Neural Attention 관련 논문
1. [Top-down neural attention by Excitation Backprop](https://github.com/pinkrespect/papers_I_read/blob/master/XAI69-Top-down%20Neurla%20Attention%20By%20Excitation%20Backprop%20%EC%9A%94%EC%95%BD_2019.12.10_%EA%B9%80%EC%96%91%EC%84%A0.md)
  - Neural Network의 Excitation을 Back Propagation을 통해 확인할 수 있음

2. [Learning What and Where to draw](https://github.com/pinkrespect/papers_I_read/blob/master/XAI081%20Learning%20What%20and%20Where%20to%20draw%20%EC%9A%94%EC%95%BD.md)
  - GAN 관련 논문인 것처럼 보이지만 GAN은 하나의 모델일 뿐임
  - 어떻게 효과적으로 무엇을 어디에 그려야할지 학습시키는 방법

3. [Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization](https://github.com/pinkrespect/papers_I_read/blob/master/Grad-CAM%20%EB%85%BC%EB%AC%B8%20%EC%9A%94%EC%95%BD%20-%2020191205%20%EA%B9%80%EC%96%91%EC%84%A0%20ver0.01.md)
  - Neural Network의 Attention을 Gradient로 표현하고, 왜 그렇게 생각했는지 Word로 설명


## [](#Header-4) 첨언
이번 다니는 회사에서는 논문을 읽을 제대로 된 시간을 주질 않는다.<br>
한 3-4일 정도 여유를 주었으면 충분히 논문을 요약할 시간이 되었을텐데, 매우 아쉽다.


